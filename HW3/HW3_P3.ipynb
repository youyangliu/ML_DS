{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import diags\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "hw3 = loadmat('hw3data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = hw3['data']\n",
    "labels = hw3['labels']\n",
    "\n",
    "#apply affine expansion\n",
    "intercept = np.ones((np.shape(data)[0],1))\n",
    "data = np.hstack((intercept, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define objective funtion\n",
    "def obj(x, y, w):\n",
    "    b = 1/np.shape(x)[0] * np.sum(np.log(1+np.exp(x.dot(w)))  - y*((x.dot(w))))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define gradient function\n",
    "def grad(x,y,w):\n",
    "    a= np.exp(x.dot(w))\n",
    "    b1= (1/np.shape(x)[0]*np.sum((a/(1+a))*x-y*x,axis=0)).reshape((np.shape(w)[0],1))\n",
    "    return b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations:  4658 \n",
      "Beta:  \n",
      " [[-9.57915501e-01]\n",
      " [-7.42639405e-03]\n",
      " [ 2.09527203e+00]\n",
      " [-1.12412797e-03]] \n",
      "Objective function value:  0.6506399987127174 \n",
      "\n",
      "CPU times: user 8.04 s, sys: 124 ms, total: 8.17 s\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "#run the gradient descent algorithm on the original data\n",
    "\n",
    "#reset initial beta\n",
    "beta = np.zeros((np.shape(data)[1],1))\n",
    "\n",
    "#define stopping condition\n",
    "threshold = 0.65064\n",
    "\n",
    "l = grad(data,labels,beta)\n",
    "o = obj(data,labels,beta)\n",
    "count = 0\n",
    "\n",
    "while ( o > threshold):\n",
    "    eta=1\n",
    "    while(obj(data,labels,beta - eta*l)> (o - 1/2*eta*(np.sum(l*l)))):\n",
    "        eta = eta/2\n",
    "    \n",
    "    beta = beta - eta*l\n",
    "    l = grad(data,labels,beta)\n",
    "    o = obj(data,labels,beta)\n",
    "    count += 1\n",
    "\n",
    "print(\"Number of iterations: \", count, \"\\n\"\n",
    "      \"Beta: \", '\\n', beta, '\\n'\n",
    "      \"Objective function value: \", o, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = data.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.place(y_hat, y_hat<=0, 0)\n",
    "np.place(y_hat, y_hat>0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37036133])"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainig error rate\n",
    "1-sum(y_hat == labels)/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAACsCAYAAABmUVoTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSR3/HxycbDQ4UzaLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182c3f6320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#observe the distributions of input features. \n",
    "#The first graph is just the distribution of intercept, therefore all 1's.\n",
    "\n",
    "fig = plt.figure(figsize=(10,2.5))\n",
    "\n",
    "for i in range(np.shape(data)[1]):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.hist(data[:,i])\n",
    "    plt.xlabel(i)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear transformation scaler\n",
    "scaler = np.diag(1/data.max(axis=0))\n",
    "data_sc = data.dot(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations:  377 \n",
      "Beta:  \n",
      " [[-0.95537672]\n",
      " [-0.1534398 ]\n",
      " [ 2.09978214]\n",
      " [-0.02757352]] \n",
      "Objective function value:  0.6506399857204013 \n",
      "Gradient value:  \n",
      " [[ 1.92804044e-04]\n",
      " [-9.78866902e-05]\n",
      " [-1.65559103e-04]\n",
      " [-1.04159829e-04]] \n",
      "\n",
      "CPU times: user 556 ms, sys: 11.5 ms, total: 568 ms\n",
      "Wall time: 339 ms\n"
     ]
    }
   ],
   "source": [
    "#run the gradient descent algoritm on linearly tranformed data\n",
    "\n",
    "#reset initial beta\n",
    "beta = np.zeros((np.shape(data)[1],1))\n",
    "\n",
    "#stopping condition\n",
    "threshold = 0.65064\n",
    "\n",
    "l = grad(data_sc,labels,beta)\n",
    "o = obj(data_sc,labels,beta)\n",
    "count = 0\n",
    "\n",
    "while ( o > threshold):\n",
    "    eta=1\n",
    "    while(obj(data_sc,labels,beta - eta*l)> (o - 1/2*eta*(l.transpose()[0].dot(l)))):\n",
    "        eta = eta/2\n",
    "    \n",
    "    beta = beta - eta*l\n",
    "    l = grad(data_sc,labels,beta)\n",
    "    o = obj(data_sc,labels,beta)\n",
    "    count +=1\n",
    "\n",
    "print(\"Number of iterations: \", count, \"\\n\"\n",
    "      \"Beta: \", '\\n', beta, '\\n'\n",
    "      \"Objective function value: \", o, \"\\n\"\n",
    "      \"Gradient value: \",'\\n', l, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37060547])"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training error rate\n",
    "y_hat=data_sc.dot(beta)\n",
    "np.place(y_hat, y_hat<=0, 0)\n",
    "np.place(y_hat, y_hat>0, 1)\n",
    "1 - sum(y_hat == labels)/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data into training and validation set using floor(0.8n) as the cutoff number.\n",
    "\n",
    "sel = int(np.floor(0.8*np.shape(data)[0]))\n",
    "x_train = data[0:sel,:]\n",
    "y_train = labels[0:sel]\n",
    "x_val = data[sel:np.shape(data)[0],:]\n",
    "y_val = labels[sel:np.shape(data)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations:  512 \n",
      "Validation error rate:  [0.38292683] \n",
      "Objective function value:  0.6550696234597402 \n",
      "\n",
      "CPU times: user 1.25 s, sys: 26.4 ms, total: 1.28 s\n",
      "Wall time: 706 ms\n"
     ]
    }
   ],
   "source": [
    "#run the gradient descent algorithm on the original data using stopping condition defined by part (d).\n",
    "\n",
    "#reset intial beta\n",
    "beta = np.zeros((np.shape(data)[1],1))\n",
    "\n",
    "#threshold = 0.65064\n",
    "l = grad(x_train,y_train,beta)\n",
    "o = obj(x_train,y_train,beta)\n",
    "p=0\n",
    "errors =[1]\n",
    "count = 0\n",
    "\n",
    "while 1:    \n",
    "    eta=1\n",
    "    while(obj(x_train,y_train,beta - eta*l)> (o - 1/2*eta*(np.sum(l*l)))):\n",
    "        eta = eta/2\n",
    "    \n",
    "    if 2**p == count:\n",
    "        y_hat=x_val.dot(beta)\n",
    "        np.place(y_hat, y_hat<=0, 0)\n",
    "        np.place(y_hat, y_hat>0, 1)\n",
    "        error = 1 - sum(y_hat.astype(int) == y_val)/len(y_val)\n",
    "        if (error > 0.99*(min(errors)) and count >=32):\n",
    "            break\n",
    "        p+=1\n",
    "        errors.append(error)\n",
    "    \n",
    "    beta = beta - eta*l\n",
    "    l = grad(x_train,y_train,beta)\n",
    "    o = obj(x_train,y_train,beta)\n",
    "    count += 1\n",
    "    \n",
    "print(\"Number of iterations: \", count, \"\\n\"\n",
    "      \"Validation error rate: \", error, '\\n'\n",
    "      \"Objective function value: \", o, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36782662])"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat=x_train.dot(beta)\n",
    "np.place(y_hat, y_hat<=0, 0)\n",
    "np.place(y_hat, y_hat>0, 1)\n",
    "1 - sum(y_hat == y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linearly transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel = int(np.floor(0.8*np.shape(data)[0]))\n",
    "x_sc_train = data_sc[0:sel,:]\n",
    "y_train = labels[0:sel]\n",
    "x_sc_val = data_sc[sel:np.shape(data)[0],:]\n",
    "y_val = labels[sel:np.shape(data)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations:  32 \n",
      "Validation error rate:  [0.37926829] \n",
      "Objective function value:  0.6647592521378464 \n",
      "\n",
      "CPU times: user 54.1 ms, sys: 3 ms, total: 57.1 ms\n",
      "Wall time: 44.7 ms\n"
     ]
    }
   ],
   "source": [
    "#run the gradient descent algorithm on the linearly transformed data using stopping condition defined by part (d).\n",
    "\n",
    "beta = np.zeros((np.shape(data)[1],1))\n",
    "\n",
    "#threshold = 0.65064\n",
    "l = grad(x_sc_train,y_train,beta)\n",
    "o = obj(x_sc_train,y_train,beta)\n",
    "p=0\n",
    "errors =[1]\n",
    "count = 0\n",
    "\n",
    "while 1:\n",
    "    eta=1\n",
    "    while(obj(x_sc_train,y_train,beta - eta*l)> (o - 1/2*eta*(l.transpose()[0].dot(l)))):\n",
    "        eta = eta/2\n",
    "    \n",
    "    if 2**p == count:\n",
    "        y_hat=x_sc_val.dot(beta)\n",
    "        np.place(y_hat, y_hat<=0, 0)\n",
    "        np.place(y_hat, y_hat>0, 1)\n",
    "        error = 1 - sum(y_hat == y_val)/len(y_val)\n",
    "        if (error > 0.99*(min(errors)) and count >=32):\n",
    "            break\n",
    "        p+=1\n",
    "        errors.append(error)\n",
    "    \n",
    "    beta = beta - eta*l\n",
    "    l = grad(x_sc_train,y_train,beta)\n",
    "    o = obj(x_sc_train,y_train,beta)\n",
    "    count +=1\n",
    "    \n",
    "print(\"Number of iterations: \", count, \"\\n\"\n",
    "      \"Validation error rate: \", error, '\\n'\n",
    "      \"Objective function value: \", o, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36721612])"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#traning error rate\n",
    "y_hat=x_sc_train.dot(beta)\n",
    "np.place(y_hat, y_hat<=0, 0)\n",
    "np.place(y_hat, y_hat>0, 1)\n",
    "1 - sum(y_hat == y_train)/len(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
